{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDKqQJFeSW0f"
      },
      "source": [
        "# Introduction to GPT-2 Version 3\n",
        "by Eugenio Tufino, University of Padova, December, 2024\n",
        "\n",
        "updated version May 6, 2025\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/etufino/Introduction-to-LLM/blob/main/IntroductiontoLLM_Activity_2_GPT_2_V3.ipynb)\n",
        "\n",
        "\n",
        "In this notebook, we present GPT-2, a simplified model compared to the state-of-the-art generative models such as  GPT-4 or OpenAI o1, the model just released. GPT-2 is based on the **Generative Pre-trained Transformer** architecture and was released by OpenAI in **2019**. It uses a relatively simple mechanism to generate text, predicting the next word in a sequence based on the given context.\n",
        "\n",
        "Therefore unlike Word2Vec, where as we have seen word vectors are static (the same vector represents a word regardless of its usage), GPT-2 **dynamically adjusts the word representation** based on its context. This allows GPT-2 to better capture the meaning of words in different situations, resulting in more coherent and contextually accurate text generation.\n",
        "\n",
        "We will explore this model to understand, in a simplified way, how this kind of generative AI works, focusing on its word generation process and how prompts affect its output.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "GPT stands for **Generative Pre-trained Transformer**, highlighting its key features:\n",
        "- Generative: The model generates coherent and meaningful text.\n",
        "- Pre-trained: It is trained on large datasets before being fine-tuned for specific tasks.\n",
        "- Transformer: It uses the Transformer architecture for efficient and context-aware text generation.\n",
        "\n",
        "\n",
        "Further reading:\n",
        "1. https://en.wikipedia.org/wiki/GPT-2\n",
        "2. https://huggingface.co/docs/transformers/en/model_doc/gpt2\n",
        "3. The Transformer architecture was introduced here: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). \"Attention Is All You Need.\" Advances in Neural Information Processing Systems\n",
        "4.. https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji9zO4BzNLAN"
      },
      "source": [
        "### Import the libraries for using GPT-2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install specific versions of PyTorch and Transformers.\n",
        "# This helps create a stable environment and avoid common version conflicts\n",
        "# that can occur with other libraries students might have or install\n",
        "!pip install --quiet \\\n",
        "  torch==2.6.0 \\\n",
        "  \"transformers>=4.41.0,<5.0.0\""
      ],
      "metadata": {
        "id": "Bi6oRlgRS70C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWFpRNz12cvF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bu8H4t_YQo5"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "#model_name = \"gpt2\"  # This is the smallest size. You can also use 'gpt2-medium', 'gpt2-large'.\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# The tokenizer processes input text into tokens and converts tokens back to text\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name) # Load the pre-trained tokenizer for GPT-2\n",
        "# The tokenizer processes input text into tokens and converts tokens back to text\n",
        "#Load the pre-trained GPT-2 language model\n",
        "# The model generates text by predicting the next token in a sequence\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "print(\"GPT-2 model and tokenizer loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxfcr80CJO1g"
      },
      "source": [
        "### What does a 'tokenizer' mean, and what is a token?\n",
        "Check this website (for the latest version of GPT): [Link](https://platform.openai.com/tokenizer)\n",
        "\n",
        "Try to tokenize the following text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xySdSor5TuB3"
      },
      "outputs": [],
      "source": [
        "# Text to tokenize\n",
        "text = \"What is AI?\"\n",
        "# Convert text into tokens\n",
        "tokenized_text = tokenizer.tokenize(text)\n",
        "# Ġ simply indicates a space precedes the token\n",
        "print(\"Tokenized text:\", tokenized_text)\n",
        "tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "# Output the tokens\n",
        "print(tokens)  #print the token Ids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: \"Ġ\" is a special character used by the tokenizer, Ġis  means the token \"is\" was preceded by a space in your original text."
      ],
      "metadata": {
        "id": "kHHL4zGcBRml"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt4J77PwQwZN"
      },
      "source": [
        "### Info about the model GPT-2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7fbfpFgT3a5"
      },
      "outputs": [],
      "source": [
        "# Check vocabulary size and vector size from the model's configuration\n",
        "vocab_size = model.config.vocab_size\n",
        "vector_size = model.config.n_embd\n",
        "\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "print(\"Vector size (number of dimensions):\", vector_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMD_ccvb6PF0"
      },
      "source": [
        "## Generating text with GPT-2\n",
        "\n",
        "The following code contains instructions for GPT-2 to generate text from an initial prompt (You can choose by uncommenting). Some parameters values are setted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Swdw4SZqZQ_1"
      },
      "outputs": [],
      "source": [
        "#prompt = \"The importance of communication in modern society\"\n",
        "\n",
        "# Introduce some physics-related prompts\n",
        "\n",
        "#prompt=\"In physics, Newton's laws describe the relationship between motion and forces\"\n",
        "prompt = \"In physics, energy is the ability to\"\n",
        "#prompt=\"In classical mechanics, Newton's First Law states that...\"\n",
        "#prompt=\"In physics, the speed of light\"\n",
        "#prompt=\"The theory of relativity, proposed by Albert Einstein, revolutionized our understanding of space and time.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBT-G5Sgaq1P"
      },
      "outputs": [],
      "source": [
        "# Tokenize the prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "# Create an attention mask\n",
        "attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask=attention_mask,  # Specify the attention mask\n",
        "    max_length=80,#Maximum length of the generated sequence\n",
        "    num_return_sequences=1,# Number of different sequences to generate\n",
        "    no_repeat_ngram_size=3,  # # Avoids repeated bigrams, see Annex A\n",
        "    temperature=0.8,\n",
        "    top_k=50, #Limits sampling to the top 50 tokens by probability, reducing unlikely choices\n",
        "    top_p=0.95,# Enables nucleus sampling, selecting tokens with cumulative probability ≤ 0.95\n",
        "    do_sample=True,  # Enable sampling\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lHNDYWKB9nd"
      },
      "source": [
        "In a previous session (with GPT-2 **small model**) with the initial prompt \"In physics, energy is the ability to\", I obtained the following text:\n",
        "> In physics, energy is the ability to move matter in the universe. This is why the earth is flat. But the way the world works is different.The earth orbits the sun, which is a reflection of the moon, and then it turns around and turns the other way around. The solar system's rotation slows down when the Earth spins, so the solar wind is pulled in two directions. It takes a lot of energy to turn the two sides of a circle around each other...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioNCzcl8a8ed"
      },
      "source": [
        "## Questions\n",
        "1. How coherent is the generated text with the given prompt?\n",
        "2. Does the model truly understand the concepts, or is it merely generating plausible sentences?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7H89VIwBQwj"
      },
      "source": [
        "GPT-2 generates text dynamically by predicting the next word in a sequence based on its context (through the Transformer architecture). It can produce  nonsensical or inaccurate content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkK3JNhKaAmr"
      },
      "source": [
        "### Parameter you can vary to improve the generated text:\n",
        "**temperature:** Controls the randomness of text generation. A lower value produces more predictable text, while a higher value increases creativity.\n",
        "**max_length**: Adjusts the length of the generated text.\n",
        "**top_k:** Regulate the selection of probable words at each step.\n",
        "\n",
        "(see also Annex A below)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ADOpnk8DIaH"
      },
      "source": [
        "**Exercise 1** Adjust the temperature. Experiment with different calues of temperature to improve the quality of GPT-2’s generated output. Observe how it influences the behavior of the model.\n",
        "\n",
        " Which temperature value produces the most meaningful text for your physics context? Write down the produced output too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJgma7fxCTEo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvOLaT-9vOes"
      },
      "source": [
        "**Exercise 2**  Try the exercise again with a more physics-specific prompt. Report here the prompt and the output with the choice of parameters made.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BiN5govJEEgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVP4Smnxq9Ql"
      },
      "source": [
        "### Analyze the probabilities for the next token.\n",
        "This allows us to 'look under the hood' and understand how the model assigns probabilities to different options. We can read the probabilities of the next word after the given prompt.\n",
        "\n",
        "prompt = \"\"The importance of communication in modern society\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxRZAu6HqvlA"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "# Define a prompt\n",
        "prompt = \"The importance of communication in modern society\"\n",
        "#physics related prompt = \"The speed of light\"\n",
        "\n",
        "# Encode the prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "# Forward pass through the model to get logits\n",
        "outputs = model(input_ids)\n",
        "logits = outputs.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
        "\n",
        "# Get the logits for the last token in the input\n",
        "last_token_logits = logits[0, -1, :]  # Shape: (vocab_size,)\n",
        "\n",
        "# Set the temperature\n",
        "temperature = 0.8 # Lower = less random, higher = more random/creative\n",
        "\n",
        "adjusted_logits = last_token_logits / temperature\n",
        "probs = F.softmax(adjusted_logits, dim=-1)\n",
        "\n",
        "# We set top_k=30 to show the 30 most probable options for the next token\n",
        "top_k = 30\n",
        "top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
        "\n",
        "print(\"The top {} most probable options for the next token are:\\n\".format(top_k))\n",
        "# Decode and print the top words with their probabilities\n",
        "for i in range(top_k):\n",
        "    word = tokenizer.decode(top_k_indices[i].item())\n",
        "    prob = top_k_probs[i].item()\n",
        "    print(f\"{word}: {prob:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can make a histogram"
      ],
      "metadata": {
        "id": "Z1pHMWVeFKuX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gly-2QJ-4OcH"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Decode the words and limit to top 10\n",
        "words = [tokenizer.decode(idx.item()) for idx in top_k_indices[:10]]\n",
        "probs = top_k_probs[:10].tolist()\n",
        "\n",
        "# Make punctuation more readable\n",
        "words = ['\"COMMA\"' if w == ',' else '\"PERIOD\"' if w == '.' else w for w in words]\n",
        "\n",
        "# Create the histogram\n",
        "plt.figure(figsize=(12, 8))  # Increased figure size\n",
        "plt.bar(range(len(words)), probs, align='center')\n",
        "plt.xticks(range(len(words)), words, rotation=45, ha='right', fontsize=16)  # Increased font size\n",
        "plt.xlabel('Words', fontsize=20)\n",
        "plt.ylabel('Probability', fontsize=20)\n",
        "plt.ylim(0.0, 0.9)\n",
        "#plt.title('Probabilities of the 10 Most Likely Next Words', fontsize=16)\n",
        "\n",
        "# Add values above each bar\n",
        "for i, v in enumerate(probs):\n",
        "    plt.text(i, v, f'{v:.4f}', ha='center', va='bottom', fontsize=15)\n",
        "\n",
        "plt.savefig('Hist_T0.8.png', dpi=400)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_sHeihJra7g"
      },
      "source": [
        "**Exercise 2:** Try to change the \"temperature\" parameter and see if there are any differences\n",
        "\n",
        "**Exercise 3:** You can continue the construction of the sentence, word by word..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnNUKfR5tR20"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0bAcTSttT5M"
      },
      "source": [
        "### Iterating the process\n",
        "Iterate the process changing only the temperature, the max_words and top_k parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY4Wb_ZKtSK5"
      },
      "outputs": [],
      "source": [
        "# @title Testo del titolo predefinito\n",
        "\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "# Set parameters\n",
        "max_words = 100  # Maximum number of words to generate\n",
        "temperature = 0.7\n",
        "top_k = 50\n",
        "\n",
        "# Generate word by word\n",
        "for _ in range(max_words):\n",
        "    # Forward pass to get logits\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
        "\n",
        "    # Get the logits for the last token\n",
        "    last_token_logits = logits[0, -1, :]\n",
        "\n",
        "    # Apply temperature\n",
        "    adjusted_logits = last_token_logits / temperature\n",
        "\n",
        "    # Convert logits to probabilities\n",
        "    probs = F.softmax(adjusted_logits, dim=-1)\n",
        "\n",
        "    # Sample the next token\n",
        "    next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "    # Append the token to the input_ids\n",
        "    input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=1)\n",
        "\n",
        "    # Decode the current output and print\n",
        "    generated_text = tokenizer.decode(input_ids[0])\n",
        "    print(generated_text)\n",
        "\n",
        "# Final generated paragraph\n",
        "print(\"\\nFinal Generated Paragraph:\")\n",
        "print(tokenizer.decode(input_ids[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeWC8SXByiXw"
      },
      "source": [
        "**Exercise: Try to improve the overall output with the following measures**\n",
        "\n",
        "\n",
        "\n",
        "1. Explore different prompts: You can test different prompts, specific to physics concepts, to observe how the behaviour of the model changes.\n",
        "2. You can generalize the code to consider the parameter no_repeat_ngram_size, top_p (see Annex). You can can observe how these parameters influence the diversity and coherence of the generated text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji2H2jKvsGY8"
      },
      "source": [
        "### Congratulations! You have completed the Jupyter Notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aCep9M09v6d"
      },
      "source": [
        "## Annex Parameters for text generation\n",
        "\n",
        "### **1. `no_repeat_ngram_size`**\n",
        "\n",
        "What is an n-gram?\n",
        "\n",
        "An n-gram is a sequence of n consecutive tokens (words or subwords) in text. For example:\n",
        "\n",
        "    1-gram (Unigram): [\"The\", \"cat\", \"is\", \"on\", \"the\", \"mat\"]\n",
        "    2-gram (Bigram): [\"The cat\", \"cat is\", \"is on\", \"on the\", \"the mat\"]\n",
        "    \n",
        "- **Definition**: Prevents the repetition of n-grams (sequences of `n` consecutive tokens) in the generated text.\n",
        "- **Purpose**: Ensures variety in the output by disallowing repetitive phrases.\n",
        "- **Example**:\n",
        "  - With `no_repeat_ngram_size=2`: The bigram \"the cat\" cannot appear more than once in the text.\n",
        "  - Without it: Text may loop or repeat, like \"The cat is on the mat. The cat is on the mat.\"\n",
        "\n",
        "\n",
        "\n",
        "### **2. `temperature`**\n",
        "- **Definition**: Controls the randomness of token selection during generation.\n",
        "- **Purpose**: Adjusts the \"creativity\" of the output.\n",
        "- **Behavior**:\n",
        "  - Lower values (e.g., `0.5`): Predictable and conservative text.\n",
        "  - Higher values (e.g., `1.5`): More creative but can lead to incoherence.\n",
        "\n",
        "\n",
        "\n",
        "### **3. `top_k`**\n",
        "- **Definition**: Limits sampling to the top-k most probable tokens.\n",
        "- **Purpose**: Ensures the model selects tokens from a fixed number of high-probability options.\n",
        "- **Behavior**:\n",
        "  - Lower values (e.g., `10`): More focused and deterministic text.\n",
        "  - Higher values (e.g., `50`): Allows greater diversity but risks introducing randomness.\n",
        "\n",
        "\n",
        "### **4. `top_p` (Nucleus Sampling)**\n",
        "- **Definition**: Chooses tokens whose cumulative probability is ≤ `top_p`.\n",
        "- **Purpose**: Dynamically adjusts the diversity of token selection.\n",
        "- **Behavior**:\n",
        "  - Lower values (e.g., `0.8`): Limits to the most likely tokens, producing focused text.\n",
        "  - Higher values (e.g., `0.95`): Includes more tokens, allowing for greater creativity.\n",
        "\n",
        "### **5. `max_length`**\n",
        "- **Definition**: Sets the maximum number of tokens in the generated sequence, including the input.\n",
        "- **Purpose**: Controls how long the generated text will be.\n",
        "- **Behavior**:\n",
        "  - Short sequences (e.g., `50`): Output is concise but might be incomplete.\n",
        "  - Long sequences (e.g., `200`): Risk of losing coherence or repeating patterns.\n",
        "\n",
        "\n",
        "### **6. `pad_token_id`**\n",
        "- **Definition**: Specifies the token used for padding sequences to a uniform length.\n",
        "- **Purpose**: Ensures the model handles sequences with padding properly during generation.\n",
        "- **Common Value**: Set to the End-of-Sequence (EOS) token in GPT-2 (`tokenizer.eos_token_id`).\n",
        "\n",
        "\n",
        "### **7. `attention_mask`**\n",
        "- **Definition**: A binary mask indicating which tokens in the input are valid (1) and which are padding (0).\n",
        "- **Purpose**: Ensures the model only attends to meaningful tokens during processing.\n",
        "- **Behavior**:\n",
        "  - Without it: The model may produce unreliable results, especially with padded sequences.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0CQSah1Cg8gSsGhfD/c5J"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}